#!/usr/bin/env python3

"""Plan-Execute Agent for Trajectory Collection (Qwen Planning + AgentCortex Execution).

This provides a drop-in replacement for QwenTestAgent that uses
Qwen LLM for planning + AgentCortex Execution Service, maintaining the
same interface for trajectory collection while generating training data
for the planning agent.
"""

import json
from typing import List, Dict, Any, Optional

from config import LLMConfig, GenerationOptions as LLMGenOpts
from core.llm_client import sync_request_llm
from core.agentcortex.execution_service import ExecutionService
from agent_types.planning import PlanningRequest
from agent_types.execution import ToolExecutingRequest  
from agent_types.common import SessionMemory, ChatMessage, Plan, ToolCalling


class PlanExecuteAgent:
    """Plan-Execute agent using Qwen for planning + AgentCortex for execution."""
    
    def __init__(
        self, 
        llm_cfg: LLMConfig, 
        generation_opts: LLMGenOpts = None, 
        tool_names: Optional[List[str]] = None,
        executor_url: str = None
    ):
        """Initialize Plan-Execute agent with Qwen planning + AgentCortex execution.
        
        Args:
            llm_cfg: LLM configuration for Qwen planning
            generation_opts: Generation options for Qwen
            tool_names: Available tool names (for compatibility)
            executor_url: AgentCortex executor URL
        """
        self.llm_cfg = llm_cfg
        self.generation_opts = generation_opts or LLMGenOpts()
        
        # Initialize AgentCortex execution service (keep this!)
        if executor_url is None:
            from config.agentcortex_config import get_agentcortex_config
            config = get_agentcortex_config()
            executor_url = config.execution_url
        
        self.execution_service = ExecutionService(executor_url)
        
        # Store for compatibility
        self.tool_names = tool_names or [tool.name for tool in self.execution_service.tools]
        
        # Qwen planning system prompt
        self.planning_system_prompt = (
            "‰Ω†ÊòØ‰∏Ä‰∏™ËÅîÊÉ≥ÂïÜÂüéÁöÑÊô∫ËÉΩÂä©ÊâãËßÑÂàíÊ®°Âùó„ÄÇÊ†πÊçÆÁî®Êà∑Êü•ËØ¢ÂíåÂèØÁî®Â∑•ÂÖ∑ÔºåÁîüÊàêÂÖ∑‰ΩìÁöÑÊâßË°åËÆ°Âàí„ÄÇ\n"
            "‰Ω†ÈúÄË¶ÅÔºö\n"
            "1. ÂàÜÊûêÁî®Êà∑ÈúÄÊ±Ç\n"
            "2. ÈÄâÊã©ÂêàÈÄÇÁöÑÂ∑•ÂÖ∑Êù•Êª°Ë∂≥ÈúÄÊ±Ç\n" 
            "3. Â¶ÇÊûúÈúÄË¶ÅË∞ÉÁî®Â∑•ÂÖ∑Ôºå‰ΩøÁî®function_callÊ†ºÂºè\n"
            "4. Â¶ÇÊûú‰∏çÈúÄË¶ÅÂ∑•ÂÖ∑ÔºåÁõ¥Êé•ÂõûÂ§çÁî®Êà∑\n"
            "5. ‰øùÊåÅ‰∏ì‰∏ö„ÄÅÂáÜÁ°Æ„ÄÅÊúâÂ∏ÆÂä©ÁöÑËØ≠Ë∞É\n\n"
            "ÂèØÁî®ÁöÑÂ∑•ÂÖ∑ÂåÖÊã¨ÂïÜÂìÅÊêúÁ¥¢„ÄÅËÆ¢ÂçïÊü•ËØ¢„ÄÅÁî®Êà∑‰ø°ÊÅØËé∑ÂèñÁ≠âËÅîÊÉ≥ÂïÜÂüéÁõ∏ÂÖ≥ÂäüËÉΩ„ÄÇ"
        )
        
    def respond(self, history: List[dict], tools_schema: List[dict]) -> List[dict]:
        """Generate agent response using Qwen planning + AgentCortex execution.
        
        This maintains the same interface as QwenTestAgent.respond() but uses
        Qwen for planning and AgentCortex for execution.
        
        Args:
            history: OpenAI-style message history
            tools_schema: Tool schemas 
            
        Returns:
            List of new messages generated by the agent
        """
        try:
            # Extract current user query and build chat history
            user_query = ""
            chat_history = []
            
            for msg in history:
                if msg.get("role") == "user":
                    user_query = msg.get("content", "")
                    chat_history.append(ChatMessage(
                        role="user",
                        content=msg.get("content", ""),
                        turn_id=len(chat_history) + 1
                    ))
                elif msg.get("role") == "assistant":
                    chat_history.append(ChatMessage(
                        role="assistant", 
                        content=msg.get("content", ""),
                        turn_id=len(chat_history) + 1
                    ))
            
            # Build messages for Qwen planning
            planning_messages = [
                {"role": "system", "content": self.planning_system_prompt}
            ]
            
            # Add conversation history
            for msg in history:
                if msg.get("role") != "system":  # Skip system messages
                    planning_messages.append({
                        "role": msg.get("role"),
                        "content": msg.get("content", "")
                    })
            
            # Call Qwen LLM for planning
            print("üß† Using Qwen LLM for planning...")
            completion = sync_request_llm(
                self.llm_cfg, 
                planning_messages, 
                generation_config=self.generation_opts,
                tools=tools_schema  # Provide tools to Qwen
            )
            
            response_message = completion.choices[0].message
            messages = []
            
            # Check if Qwen made function calls
            if hasattr(response_message, 'function_call') and response_message.function_call:
                print("üîß Qwen planned function call, converting to Plan object...")
                
                # Convert Qwen function call to Plan object
                function_call = response_message.function_call
                tool_name = function_call.name
                
                # Parse arguments
                raw_args = function_call.arguments
                if isinstance(raw_args, str):
                    try:
                        args_dict = json.loads(raw_args)
                    except json.JSONDecodeError:
                        args_dict = {}
                else:
                    args_dict = raw_args or {}
                
                # Create ToolCalling object
                tool_calling = ToolCalling(
                    name=tool_name,
                    arguments=args_dict
                )
                
                # Create Plan object  
                plan = Plan(
                    tool_callings=[tool_calling],
                    content=getattr(response_message, 'content', '') or ""
                )
                
                # Add assistant message with function call
                function_call_msg = {
                    "role": "assistant",
                    "function_call": {
                        "name": tool_name,
                        "arguments": json.dumps(args_dict, ensure_ascii=False)
                    },
                    "content": ""
                }
                messages.append(function_call_msg)
                
                # Execute via AgentCortex execution service
                print("‚ö° Executing via AgentCortex execution service...")
                
                # Create realistic default_args for AgentCortex
                default_args = {
                    "user_info": {
                        "uid": "13716255679", 
                        "user_identity": 1, 
                        "available_num": 0.0, 
                        "current_amount": "0", 
                        "enterprise_name": "", 
                        "future_expire_num": 0.0, 
                        "level_name": "", 
                        "entry_source": "shop", 
                        "user_province": ""
                    },
                    "trace_id": "qwen_planning_trajectory",
                    "uid": "13716255679",
                    "terminal": "1",
                    "latitude": "23.89447712420573",
                    "longitude": "106.6172117534938",
                    "device_ip": "117.183.16.69",
                    "get_position_permission": "agree",
                    "event": "",
                    "bind_mobile_id": 0,
                    "query": user_query,
                    "chat_history": [msg.model_dump() for msg in chat_history],
                    "mentions": []
                }
                
                # Create session memory
                session_memory = SessionMemory(
                    chat_history=chat_history,
                    mentions=[]
                )
                
                # Execute the tool via AgentCortex
                execution_request = ToolExecutingRequest(
                    plan=plan,
                    task=user_query,
                    session_memory=session_memory,
                    default_args=default_args
                )
                
                execution_response = self.execution_service.execute_tools(execution_request)
                observation = execution_response.observation
                
                # Add function result message
                for status in observation.status:
                    if status.name == tool_name:
                        if status.result:
                            result_content = json.dumps(status.result, ensure_ascii=False) if isinstance(status.result, dict) else str(status.result)
                        elif status.error:
                            result_content = f"Error: {status.error.message}"
                        else:
                            result_content = "No result"
                        
                        function_result_msg = {
                            "role": "function",
                            "name": tool_name,
                            "content": result_content
                        }
                        messages.append(function_result_msg)
                        break
                
                # Add final response if plan has content
                if plan.content:
                    assistant_msg = {
                        "role": "assistant",
                        "content": plan.content
                    }
                    messages.append(assistant_msg)
                    
            else:
                # No function call - just return Qwen's direct response
                print("üí¨ Qwen provided direct response (no tools needed)")
                content = getattr(response_message, 'content', '') or "ÊàëÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÊù•Â∏ÆÂä©ÊÇ®„ÄÇ"
                assistant_msg = {
                    "role": "assistant",
                    "content": content
                }
                messages.append(assistant_msg)
            
            return messages
            
        except Exception as e:
            # Fallback error response
            print(f"‚ùå Error in Qwen planning: {str(e)}")
            error_msg = {
                "role": "assistant",
                "content": f"Êä±Ê≠âÔºåÊàëÈÅáÂà∞‰∫Ü‰∏Ä‰∫õÈóÆÈ¢òÔºö{str(e)}"
            }
            return [error_msg] 