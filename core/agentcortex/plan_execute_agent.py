#!/usr/bin/env python3

"""Plan-Execute Agent for Trajectory Collection (Qwen Planning + AgentCortex Execution).

This provides a drop-in replacement for QwenTestAgent that uses
Qwen LLM for planning + AgentCortex Execution Service, maintaining the
same interface for trajectory collection while generating training data
for the planning agent.
"""

import json
from typing import List, Dict, Any, Optional

from config import LLMConfig, GenerationOptions as LLMGenOpts
from core.llm_client import sync_request_llm
from core.agentcortex.execution_service import ExecutionService
from agent_types.planning import PlanningRequest
from agent_types.execution import ToolExecutingRequest  
from agent_types.common import SessionMemory, ChatMessage, Plan, ToolCalling


class PlanExecuteAgent:
    """Plan-Execute agent using Qwen for planning + AgentCortex for execution."""
    
    def __init__(
        self, 
        llm_cfg: LLMConfig, 
        generation_opts: LLMGenOpts = None, 
        tool_names: Optional[List[str]] = None,
        executor_url: str = None
    ):
        """Initialize Plan-Execute agent with Qwen planning + AgentCortex execution.
        
        Args:
            llm_cfg: LLM configuration for Qwen planning
            generation_opts: Generation options for Qwen
            tool_names: Available tool names (for compatibility)
            executor_url: AgentCortex executor URL
        """
        self.llm_cfg = llm_cfg
        self.generation_opts = generation_opts or LLMGenOpts()
        
        # Initialize AgentCortex execution service (keep this!)
        if executor_url is None:
            from config.agentcortex_config import get_agentcortex_config
            config = get_agentcortex_config()
            executor_url = config.execution_url
        
        self.execution_service = ExecutionService(executor_url)
        
        # Store for compatibility
        self.tool_names = tool_names or [tool.name for tool in self.execution_service.tools]
        
        # Qwen planning system prompt
        self.planning_system_prompt = (
            "你是一个联想商城的智能助手规划模块。根据用户查询和可用工具，生成具体的执行计划。\n"
            "你需要：\n"
            "1. 分析用户需求\n"
            "2. 选择合适的工具来满足需求\n" 
            "3. 如果需要调用工具，使用function_call格式\n"
            "4. 如果不需要工具，直接回复用户\n"
            "5. 保持专业、准确、有帮助的语调\n\n"
            "可用的工具包括商品搜索、订单查询、用户信息获取等联想商城相关功能。"
        )
        
    def respond(self, history: List[dict], tools_schema: List[dict]) -> List[dict]:
        """Generate agent response using Qwen planning + AgentCortex execution.
        
        This maintains the same interface as QwenTestAgent.respond() but uses
        Qwen for planning and AgentCortex for execution.
        
        Args:
            history: OpenAI-style message history
            tools_schema: Tool schemas 
            
        Returns:
            List of new messages generated by the agent
        """
        try:
            # Extract current user query and build chat history
            user_query = ""
            chat_history = []
            
            for msg in history:
                if msg.get("role") == "user":
                    user_query = msg.get("content", "")
                    chat_history.append(ChatMessage(
                        role="user",
                        content=msg.get("content", ""),
                        turn_id=len(chat_history) + 1
                    ))
                elif msg.get("role") == "assistant":
                    chat_history.append(ChatMessage(
                        role="assistant", 
                        content=msg.get("content", ""),
                        turn_id=len(chat_history) + 1
                    ))
            
            # Build messages for Qwen planning
            planning_messages = [
                {"role": "system", "content": self.planning_system_prompt}
            ]
            
            # Add conversation history
            for msg in history:
                if msg.get("role") != "system":  # Skip system messages
                    planning_messages.append({
                        "role": msg.get("role"),
                        "content": msg.get("content", "")
                    })
            
            # Call Qwen LLM for planning
            print("🧠 Using Qwen LLM for planning...")
            completion = sync_request_llm(
                self.llm_cfg, 
                planning_messages, 
                generation_config=self.generation_opts,
                tools=tools_schema  # Provide tools to Qwen
            )
            
            response_message = completion.choices[0].message
            messages = []
            
            # Check if Qwen made function calls
            if hasattr(response_message, 'function_call') and response_message.function_call:
                print("🔧 Qwen planned function call, converting to Plan object...")
                
                # Convert Qwen function call to Plan object
                function_call = response_message.function_call
                tool_name = function_call.name
                
                # Parse arguments
                raw_args = function_call.arguments
                if isinstance(raw_args, str):
                    try:
                        args_dict = json.loads(raw_args)
                    except json.JSONDecodeError:
                        args_dict = {}
                else:
                    args_dict = raw_args or {}
                
                # Create ToolCalling object
                tool_calling = ToolCalling(
                    name=tool_name,
                    arguments=args_dict
                )
                
                # Create Plan object  
                plan = Plan(
                    tool_callings=[tool_calling],
                    content=getattr(response_message, 'content', '') or ""
                )
                
                # Add assistant message with function call
                function_call_msg = {
                    "role": "assistant",
                    "function_call": {
                        "name": tool_name,
                        "arguments": json.dumps(args_dict, ensure_ascii=False)
                    },
                    "content": ""
                }
                messages.append(function_call_msg)
                
                # Execute via AgentCortex execution service
                print("⚡ Executing via AgentCortex execution service...")
                
                # Create realistic default_args for AgentCortex
                default_args = {
                    "user_info": {
                        "uid": "13716255679", 
                        "user_identity": 1, 
                        "available_num": 0.0, 
                        "current_amount": "0", 
                        "enterprise_name": "", 
                        "future_expire_num": 0.0, 
                        "level_name": "", 
                        "entry_source": "shop", 
                        "user_province": ""
                    },
                    "trace_id": "qwen_planning_trajectory",
                    "uid": "13716255679",
                    "terminal": "1",
                    "latitude": "23.89447712420573",
                    "longitude": "106.6172117534938",
                    "device_ip": "117.183.16.69",
                    "get_position_permission": "agree",
                    "event": "",
                    "bind_mobile_id": 0,
                    "query": user_query,
                    "chat_history": [msg.model_dump() for msg in chat_history],
                    "mentions": []
                }
                
                # Create session memory
                session_memory = SessionMemory(
                    chat_history=chat_history,
                    mentions=[]
                )
                
                # Execute the tool via AgentCortex
                execution_request = ToolExecutingRequest(
                    plan=plan,
                    task=user_query,
                    session_memory=session_memory,
                    default_args=default_args
                )
                
                execution_response = self.execution_service.execute_tools(execution_request)
                observation = execution_response.observation
                
                # Add function result message
                for status in observation.status:
                    if status.name == tool_name:
                        if status.result:
                            result_content = json.dumps(status.result, ensure_ascii=False) if isinstance(status.result, dict) else str(status.result)
                        elif status.error:
                            result_content = f"Error: {status.error.message}"
                        else:
                            result_content = "No result"
                        
                        function_result_msg = {
                            "role": "function",
                            "name": tool_name,
                            "content": result_content
                        }
                        messages.append(function_result_msg)
                        break
                
                # Add final response if plan has content
                if plan.content:
                    assistant_msg = {
                        "role": "assistant",
                        "content": plan.content
                    }
                    messages.append(assistant_msg)
                    
            else:
                # No function call - just return Qwen's direct response
                print("💬 Qwen provided direct response (no tools needed)")
                content = getattr(response_message, 'content', '') or "我需要更多信息来帮助您。"
                assistant_msg = {
                    "role": "assistant",
                    "content": content
                }
                messages.append(assistant_msg)
            
            return messages
            
        except Exception as e:
            # Fallback error response
            print(f"❌ Error in Qwen planning: {str(e)}")
            error_msg = {
                "role": "assistant",
                "content": f"抱歉，我遇到了一些问题：{str(e)}"
            }
            return [error_msg] 