# Auto MT Pipeline Configuration
# This file contains the essential settings you need to configure.
# Advanced settings are handled automatically with sensible defaults.

# =============================================================================
# LLM Service Configuration (REQUIRED)
# =============================================================================
llm:
  base_url: "https://modelfactory.lenovo.com/service-large-168-1743666716725/llm/v1"  # Your OpenAI-compatible endpoint
  model: "Qwen2.5-72B"                       # Model name or path
  api_key: "AnpXAL97M888frrAHmrbHmrdFJs27wrKca776zmZ786zwPJ6MrdJ8Csr6p6wGB7RJ99xk41VF5FHrbrk8zFh86RH8p8bfnKSNrCDqjan8rTal6j2wsxV9lZrJ7VlRfPH" # API key (any string if server ignores auth)

# =============================================================================
# Generation Parameters (OPTIONAL - fine-tune if needed)
# =============================================================================
generation:
  # Blueprint generation (creativity for diverse scenarios)
  blueprint_temperature: 1.0
  blueprint_max_tokens: 8192
  
  # Trajectory collection - customer simulator (consistency for realistic behavior)
  trajectory_temperature: 0.4
  trajectory_max_tokens: 4096
  
  # Retail assistant agent (balance between helpful and deterministic)
  assistant_temperature: 0.3
  assistant_max_tokens: 4096
  
  # Request timeout for all LLM calls (seconds)
  timeout: 120

# =============================================================================
# Pipeline Settings (OPTIONAL - adjust for your needs)
# =============================================================================
pipeline:
  max_blueprint_attempts: 5  # Max retries for blueprint generation
  bon_n: 3                   # Best-of-N sampling for trajectory collection
  debug: true                # Enable debug output for development

 